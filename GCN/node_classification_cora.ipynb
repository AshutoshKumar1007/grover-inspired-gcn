{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90cd25fb",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ccd633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc8fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee71e7",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee141e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = Planetoid(root=\"data\", name=\"Cora\")\n",
    "data = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c966df40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d667685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.train_mask[data.train_mask == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4dced8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.train_mask[data.val_mask == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90ba197e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf50d5",
   "metadata": {},
   "source": [
    "**Building Adjacency Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acb66d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10556])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = data.num_nodes \n",
    "edge_index =  data.edge_index\n",
    "\n",
    "A = torch.zeros((N,N))\n",
    "A[edge_index[0], edge_index[1]] = 1\n",
    "A = A + A.T\n",
    "A[A > 1] = 1\n",
    "A[A == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f799b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = torch.eye(N)\n",
    "A_hat = A + I # adding self loops\n",
    "\n",
    "D_hat = torch.diag(A_hat.sum(dim = 1))\n",
    "D_hat_inv_sqrt = torch.linalg.inv(torch.sqrt(D_hat))\n",
    "\n",
    "A_norm = D_hat_inv_sqrt @ A_hat @ D_hat_inv_sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb54383",
   "metadata": {},
   "source": [
    "**GCN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa1c0d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "  def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "    super().__init__()\n",
    "    self.W1 = torch.nn.Parameter(torch.randn(in_dim, hidden_dim))\n",
    "    self.W2 = torch.nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "    self.W3 = torch.nn.Parameter(torch.randn(hidden_dim, out_dim))\n",
    "\n",
    "  def forward(self, A_norm , X):\n",
    "    H = torch.relu(A_norm @ X @ self.W1) # 1st Aggreation and projection\n",
    "    H = A_norm @ H @ self.W2 # 2nd Aggregation & projection\n",
    "    H = A_norm @ H @ self.W3\n",
    "    return H\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cfb48f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 2708])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7239f066",
   "metadata": {},
   "source": [
    "**Train Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5101b6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 26.0368 | Train Acc 0.121\n",
      "Epoch 20 | Loss 21.9397 | Train Acc 0.121\n",
      "Epoch 40 | Loss 18.6037 | Train Acc 0.129\n",
      "Epoch 60 | Loss 15.9398 | Train Acc 0.136\n",
      "Epoch 80 | Loss 13.8123 | Train Acc 0.150\n",
      "Epoch 100 | Loss 12.0653 | Train Acc 0.164\n",
      "Epoch 120 | Loss 10.6314 | Train Acc 0.164\n",
      "Epoch 140 | Loss 9.4499 | Train Acc 0.164\n",
      "Epoch 160 | Loss 8.4731 | Train Acc 0.171\n",
      "Epoch 180 | Loss 7.6605 | Train Acc 0.171\n",
      "Epoch 200 | Loss 6.9681 | Train Acc 0.179\n",
      "Epoch 220 | Loss 6.3824 | Train Acc 0.193\n",
      "Epoch 240 | Loss 5.8784 | Train Acc 0.193\n",
      "Epoch 260 | Loss 5.4322 | Train Acc 0.186\n",
      "Epoch 280 | Loss 5.0365 | Train Acc 0.186\n",
      "Epoch 300 | Loss 4.6873 | Train Acc 0.179\n",
      "Epoch 320 | Loss 4.3778 | Train Acc 0.179\n",
      "Epoch 340 | Loss 4.1010 | Train Acc 0.179\n",
      "Epoch 360 | Loss 3.8538 | Train Acc 0.200\n",
      "Epoch 380 | Loss 3.6324 | Train Acc 0.200\n",
      "Epoch 400 | Loss 3.4345 | Train Acc 0.214\n",
      "Epoch 420 | Loss 3.2563 | Train Acc 0.214\n",
      "Epoch 440 | Loss 3.0967 | Train Acc 0.214\n",
      "Epoch 460 | Loss 2.9559 | Train Acc 0.221\n",
      "Epoch 480 | Loss 2.8278 | Train Acc 0.236\n",
      "Epoch 500 | Loss 2.7109 | Train Acc 0.250\n",
      "Epoch 520 | Loss 2.6052 | Train Acc 0.264\n",
      "Epoch 540 | Loss 2.5087 | Train Acc 0.250\n",
      "Epoch 560 | Loss 2.4184 | Train Acc 0.250\n",
      "Epoch 580 | Loss 2.3333 | Train Acc 0.257\n",
      "Epoch 600 | Loss 2.2531 | Train Acc 0.271\n",
      "Epoch 620 | Loss 2.1803 | Train Acc 0.286\n",
      "Epoch 640 | Loss 2.1128 | Train Acc 0.293\n",
      "Epoch 660 | Loss 2.0513 | Train Acc 0.293\n",
      "Epoch 680 | Loss 1.9958 | Train Acc 0.300\n",
      "Epoch 700 | Loss 1.9459 | Train Acc 0.307\n",
      "Epoch 720 | Loss 1.9008 | Train Acc 0.329\n",
      "Epoch 740 | Loss 1.8602 | Train Acc 0.307\n",
      "Epoch 760 | Loss 1.8229 | Train Acc 0.314\n",
      "Epoch 780 | Loss 1.7888 | Train Acc 0.314\n",
      "Epoch 800 | Loss 1.7576 | Train Acc 0.314\n",
      "Epoch 820 | Loss 1.7287 | Train Acc 0.321\n",
      "Epoch 840 | Loss 1.7017 | Train Acc 0.343\n",
      "Epoch 860 | Loss 1.6766 | Train Acc 0.386\n",
      "Epoch 880 | Loss 1.6529 | Train Acc 0.393\n",
      "Epoch 900 | Loss 1.6305 | Train Acc 0.386\n",
      "Epoch 920 | Loss 1.6079 | Train Acc 0.400\n",
      "Epoch 940 | Loss 1.5859 | Train Acc 0.400\n",
      "Epoch 960 | Loss 1.5650 | Train Acc 0.407\n",
      "Epoch 980 | Loss 1.5452 | Train Acc 0.407\n",
      "Epoch 1000 | Loss 1.5272 | Train Acc 0.421\n",
      "Epoch 1020 | Loss 1.5095 | Train Acc 0.429\n",
      "Epoch 1040 | Loss 1.4918 | Train Acc 0.429\n",
      "Epoch 1060 | Loss 1.4742 | Train Acc 0.429\n",
      "Epoch 1080 | Loss 1.4580 | Train Acc 0.436\n",
      "Epoch 1100 | Loss 1.4429 | Train Acc 0.443\n",
      "Epoch 1120 | Loss 1.4289 | Train Acc 0.464\n",
      "Epoch 1140 | Loss 1.4156 | Train Acc 0.479\n",
      "Epoch 1160 | Loss 1.4029 | Train Acc 0.486\n",
      "Epoch 1180 | Loss 1.3911 | Train Acc 0.486\n",
      "Epoch 1200 | Loss 1.3798 | Train Acc 0.500\n",
      "Epoch 1220 | Loss 1.3691 | Train Acc 0.500\n",
      "Epoch 1240 | Loss 1.3587 | Train Acc 0.500\n",
      "Epoch 1260 | Loss 1.3475 | Train Acc 0.507\n",
      "Epoch 1280 | Loss 1.3366 | Train Acc 0.514\n",
      "Epoch 1300 | Loss 1.3255 | Train Acc 0.521\n",
      "Epoch 1320 | Loss 1.3147 | Train Acc 0.521\n",
      "Epoch 1340 | Loss 1.3049 | Train Acc 0.521\n",
      "Epoch 1360 | Loss 1.2953 | Train Acc 0.529\n",
      "Epoch 1380 | Loss 1.2862 | Train Acc 0.529\n",
      "Epoch 1400 | Loss 1.2775 | Train Acc 0.550\n",
      "Epoch 1420 | Loss 1.2691 | Train Acc 0.550\n",
      "Epoch 1440 | Loss 1.2609 | Train Acc 0.557\n",
      "Epoch 1460 | Loss 1.2526 | Train Acc 0.571\n",
      "Epoch 1480 | Loss 1.2440 | Train Acc 0.586\n",
      "Epoch 1500 | Loss 1.2359 | Train Acc 0.593\n",
      "Epoch 1520 | Loss 1.2282 | Train Acc 0.614\n",
      "Epoch 1540 | Loss 1.2205 | Train Acc 0.614\n",
      "Epoch 1560 | Loss 1.2126 | Train Acc 0.614\n",
      "Epoch 1580 | Loss 1.2049 | Train Acc 0.621\n",
      "Epoch 1600 | Loss 1.1966 | Train Acc 0.621\n",
      "Epoch 1620 | Loss 1.1880 | Train Acc 0.629\n",
      "Epoch 1640 | Loss 1.1794 | Train Acc 0.629\n",
      "Epoch 1660 | Loss 1.1706 | Train Acc 0.629\n",
      "Epoch 1680 | Loss 1.1607 | Train Acc 0.636\n",
      "Epoch 1700 | Loss 1.1503 | Train Acc 0.643\n",
      "Epoch 1720 | Loss 1.1404 | Train Acc 0.643\n",
      "Epoch 1740 | Loss 1.1310 | Train Acc 0.643\n",
      "Epoch 1760 | Loss 1.1216 | Train Acc 0.657\n",
      "Epoch 1780 | Loss 1.1126 | Train Acc 0.657\n",
      "Epoch 1800 | Loss 1.1036 | Train Acc 0.657\n",
      "Epoch 1820 | Loss 1.0938 | Train Acc 0.657\n",
      "Epoch 1840 | Loss 1.0804 | Train Acc 0.664\n",
      "Epoch 1860 | Loss 1.0685 | Train Acc 0.671\n",
      "Epoch 1880 | Loss 1.0557 | Train Acc 0.693\n",
      "Epoch 1900 | Loss 1.0423 | Train Acc 0.686\n",
      "Epoch 1920 | Loss 1.0308 | Train Acc 0.700\n",
      "Epoch 1940 | Loss 1.0200 | Train Acc 0.700\n",
      "Epoch 1960 | Loss 1.0090 | Train Acc 0.700\n",
      "Epoch 1980 | Loss 0.9976 | Train Acc 0.700\n",
      "Epoch 2000 | Loss 0.9860 | Train Acc 0.707\n",
      "Epoch 2020 | Loss 0.9746 | Train Acc 0.714\n",
      "Epoch 2040 | Loss 0.9637 | Train Acc 0.729\n",
      "Epoch 2060 | Loss 0.9526 | Train Acc 0.750\n",
      "Epoch 2080 | Loss 0.9413 | Train Acc 0.771\n",
      "Epoch 2100 | Loss 0.9302 | Train Acc 0.779\n",
      "Epoch 2120 | Loss 0.9190 | Train Acc 0.779\n",
      "Epoch 2140 | Loss 0.9073 | Train Acc 0.786\n",
      "Epoch 2160 | Loss 0.8955 | Train Acc 0.800\n",
      "Epoch 2180 | Loss 0.8834 | Train Acc 0.814\n",
      "Epoch 2200 | Loss 0.8674 | Train Acc 0.829\n",
      "Epoch 2220 | Loss 0.8496 | Train Acc 0.843\n",
      "Epoch 2240 | Loss 0.8333 | Train Acc 0.850\n",
      "Epoch 2260 | Loss 0.8185 | Train Acc 0.864\n",
      "Epoch 2280 | Loss 0.8044 | Train Acc 0.864\n",
      "Epoch 2300 | Loss 0.7910 | Train Acc 0.879\n",
      "Epoch 2320 | Loss 0.7783 | Train Acc 0.893\n",
      "Epoch 2340 | Loss 0.7657 | Train Acc 0.900\n",
      "Epoch 2360 | Loss 0.7537 | Train Acc 0.900\n",
      "Epoch 2380 | Loss 0.7419 | Train Acc 0.907\n",
      "Epoch 2400 | Loss 0.7312 | Train Acc 0.907\n",
      "Epoch 2420 | Loss 0.7213 | Train Acc 0.907\n",
      "Epoch 2440 | Loss 0.7120 | Train Acc 0.907\n",
      "Epoch 2460 | Loss 0.7034 | Train Acc 0.900\n",
      "Epoch 2480 | Loss 0.6950 | Train Acc 0.907\n",
      "Epoch 2500 | Loss 0.6874 | Train Acc 0.907\n",
      "Epoch 2520 | Loss 0.6803 | Train Acc 0.907\n",
      "Epoch 2540 | Loss 0.6737 | Train Acc 0.914\n",
      "Epoch 2560 | Loss 0.6675 | Train Acc 0.914\n",
      "Epoch 2580 | Loss 0.6618 | Train Acc 0.914\n",
      "Epoch 2600 | Loss 0.6564 | Train Acc 0.914\n",
      "Epoch 2620 | Loss 0.6514 | Train Acc 0.914\n",
      "Epoch 2640 | Loss 0.6468 | Train Acc 0.914\n",
      "Epoch 2660 | Loss 0.6425 | Train Acc 0.914\n",
      "Epoch 2680 | Loss 0.6386 | Train Acc 0.914\n",
      "Epoch 2700 | Loss 0.6350 | Train Acc 0.914\n",
      "Epoch 2720 | Loss 0.6317 | Train Acc 0.921\n",
      "Epoch 2740 | Loss 0.6285 | Train Acc 0.921\n",
      "Epoch 2760 | Loss 0.6256 | Train Acc 0.921\n",
      "Epoch 2780 | Loss 0.6229 | Train Acc 0.921\n",
      "Epoch 2800 | Loss 0.6203 | Train Acc 0.921\n",
      "Epoch 2820 | Loss 0.6180 | Train Acc 0.936\n",
      "Epoch 2840 | Loss 0.6156 | Train Acc 0.936\n",
      "Epoch 2860 | Loss 0.6129 | Train Acc 0.936\n",
      "Epoch 2880 | Loss 0.6104 | Train Acc 0.936\n",
      "Epoch 2900 | Loss 0.6083 | Train Acc 0.936\n",
      "Epoch 2920 | Loss 0.6065 | Train Acc 0.936\n",
      "Epoch 2940 | Loss 0.6048 | Train Acc 0.936\n",
      "Epoch 2960 | Loss 0.6032 | Train Acc 0.936\n",
      "Epoch 2980 | Loss 0.6017 | Train Acc 0.936\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GCN(in_dim = data.x.size(1),hidden_dim = 6,out_dim = dataset.num_classes).to(device)\n",
    "\n",
    "X = data.x.to(device)\n",
    "A_norm = A_norm.to(device)\n",
    "labels = data.y.to(device)\n",
    "train_mask = data.train_mask.to(device)\n",
    "test_mask = data.test_mask.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 5e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(3000):\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  out = model(A_norm, X)\n",
    "  loss = loss_fn(out[train_mask], labels[train_mask])\n",
    "\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  if epoch % 20 == 0:\n",
    "    pred = out.argmax(dim=1)\n",
    "    acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "    print(f\"Epoch {epoch} | Loss {loss:.4f} | Train Acc {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d7c58",
   "metadata": {},
   "source": [
    "**Eval loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "731d885a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7160000205039978\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(A_norm, X)\n",
    "    pred = out.argmax(dim=1)\n",
    "    test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "    print(\"Test Accuracy:\", test_acc.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"gcn_cora.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96f77c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN(in_dim = data.x.size(1),hidden_dim = 6,out_dim = dataset.num_classes).to(device)\n",
    "model.load_state_dict(torch.load(\"gcn_cora.pth\"))\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
